# 3.10学习日记
>我不喜欢边写边记笔记，我觉得很没意义，反正可以回看，直接回忆你还记得的内容就行了。

今天主要学习了以下的两个内容
- 什么是机器学习
	- 数据驱动，自动学习规律，从经验中改进
- 监督学习的部分（这个部分和我之前看的统计书有点像，statistics learning by python）
~~说起来很遗憾本来应该看完这本书的，中间又跑去摆烂了~~
	- 监督学习：从x到y，给正确答案，让机器自己去找规律
		- 主要分为两类1. regression2. classfication 这二者的区别很简单，一个是预测，可能的值是无限的。另一个就是分类，可能的值是有限个的。
- 好吧，今天没学习什么内容，今天主要做的事情是，设计学习路径和寻找练习题目。另外值得一提的是，统计的学习还是相当有必要的，建议有时间的话也可以慢慢学习。

# 3.13学习日记
今天学习了
- 无监督学习：就是没有具体的答案（无标签），让机器找到数据之中的差别
	- 聚类：clustering，分类根据数据分不一样的结果
	- 异常检测：Anomaly detection，从一群数据中找出一个比较奇怪的
	- 降维：dimensionality reduction，尽可能不损失特征的情况下，减少数据的维度
- 监督学习：线性Regression：一些对应的参数，同时因为我接触python比较少，所以今天的练习全部手打了一遍，加深一下影响。

# 3.14学习日记
>老实说，我觉得你学习效率有点低下了，每天才学这么一点内容。

- cost fuction：看了传统的line model里面如何计算他的costfunction，包括MSE和MAE两种计算方式，但是显然更常见的是我们用的是MSE，这是因为它具有两个优点，他在所有位置都可导，便于优化整个函数，同时他对于误差较大的值的惩罚力度也很大。
-    别的不说了直接开始练习项目,还是一样手打一下感受一下。今天的好少，好吧最后还是堕落了。

# 3.15学习日记
>哈哈哈虽然我老是攻击自己学习的很慢但是其实吴恩达的课程希望你一周学会的内容我还是完全学会啦~我还是不错的

1. 网课笔记：今天主要学习了梯度下降。
	1. 为什么要使用梯度下降：
		1. 简化计算：尽管在模型简单的时候，比如线性模型，你可以直接用最小二乘法来计算出costfuntion最低的时候，但是在模型越来越复杂的时候，直接求方程的解是相当复炸的，而梯度下降用一步一步的进展替代了这个复杂的过程你需要做的就是不断的重复而已。（学习好像也是这样的笑）
	2. 梯度下降(gradient descent)是什么
		1. ![[Pasted image 20250315155609.png]]
		2. 对上述公式的几个说明，J（w,b) 是之前学到的cost function 衡量误差值的部分，求对w和b的偏导的意义是得到这段函数相对于w和b的斜率，也就是下降幅度，越陡峭下降幅度就越大，α学习率，决定步调大小的，较小会浪费计算资源一直算个不停，太大的时候回导致震荡。
			1. 学习率：需要注意的有两点，这个下降的设计是相当巧妙的，即使学习率不变的情况下，下降的部分也是在越陡峭的部分下降的越多，在越接近最小值的时候下降慢慢的变得缓慢。第二就是你的设想是正确的确实可以通过先设计一个比较大的学习率后再慢慢的变小，这涉及到后面要学习的一些算法在这里就不多聊了。
			2. w，b需要注意的是，是需要一个temp_wb来作为实时更新的，而不是更新了之后再代入b中，事实上这是两周完全不同的算法。
2. 数学公式推导：其实也没做什么就简单算了一下求偏导的过程，理解了为什么每次都会更逼近costfunction的最小值。![[3.14梯度函数最小值.jpg]]
3. optionlab：太简单了，笑

# 3.18学习日记
math的部分可以通过whymachinglearn来解决
![[Pasted image 20250318210206.png]]
>时刻记住你学机器学习是为了应用机器学习，而不是为了开发更新的机器学习算法，这些会有专门学数学的人去做的。

## 网课
多特征：准确的说是线性的多变量特征，这里是采用向量化的点乘来解决的，把多变量的问题转化为了一个像之前单变量的线性模式。
向量化：带来了两个有点，1.更简洁2.能够利用并行硬件让计算显著变得更快。接着让你具体理解了为什么向量化能变得更快
## optional lab
>好烦不知道为什么今天很不学习赶紧看完休息吧。

哎弄了好久
尽管他给的数据是一个双变量的，但是只读了一个变量用之前说的线性模型做了一下拟合
值得注意的地方有
1. 数据需要标准化，尤其是他的房价没有标准化之前平方和太大了会导致爆掉。
2. 可以先选一个小一点的学习率。
3. 尽管用pd读取了数据本身最后还是得转换为np的形式，因为你函数里读的内容是np，不然又会一直报错。
![[Pasted image 20250318223302.png]]

# 3.20 学习日记
>原来周二的时候并不是我状态不好，只是因为那个作业是超纲的我把今天需要做到的内容提前做了，另外真的会有人听不懂吴恩达的机器学习吗？感觉没什么难度啊。小红书骗我
1. 网课：
	1. 多重线性回归的梯度下降，其实没什么不一样主要就是需要用dot conduct来高效的处理大规模的数据，另外对于线性回归有走正规方程，可以直接计算出线性回归的解析解，得到最优权重大，但是相应的他也有一些缺点，适应范围相对窄，同时对于大样本量，计算是相对复杂的。（和我之前提的问题是一样的）
	2. 特征缩放
		1. 为什么要特征缩放：这是因为在实际生活中，影响问题的因素并不是完全一样的，这就会导致他们的权重也不一致，而这会导致costfunction呈现一个椭圆的状态，可能会导致震荡。增加寻找到最优解的时间。
		2. 如何进行scale：一般来说有三种方法，分别是最大值缩放，平均值缩放和Z-score缩放，Z-score缩放很简单，就是如果数据本身是正态分布的就用数值减去标准差/平均值就可以了。还有一点就是最大值缩放会将所有数据缩放到0~1，而平均值缩放会把所有数据缩放到-1~1
		3. 什么时候需要进行特征缩放：由于特征缩放几乎没有什么缺点所以建议什么时候都缩放。
2. optionlab：学习了numpy和vectorization，为什么向量化比别的快，同时了解了一些numpy的基础语法。


# 3.22 学习日记
不多说随便你摆烂不摆烂你这周把该学的内容学完就行。
1. optionlab：（上次发现欠了非常多optionlab，先做做再接着学）昨天做了大概两个小时这个lab啊，但是我觉得是有意义的，主要是学习了一下numpy的操作，同时让ai出题自己尝试做了一下，同时手搓了一下多元线性模型的代码，虽然还是有很多细节做的不够好但是努力了就行。

# 3.23学习日记
因为今天要出去玩，所以先学习一会。
1. 网课：（看来一下今天的optionlab，发现有的内容还没看过网课先看网课再接着做optionlab）
	1. 检查梯度下降是否收敛，主要是通过看学习曲线，当曲线足够平滑的时候我们就可以认为他是收敛的，或者是设定一个相当小的值，当每次梯度下降减小的值小于该值就可以近似认为已经收敛了，但是这同样存在一个问题，就是多小的值才足够小，来证明收敛，综上所述，大家一般还是通过学习曲线来观察。
	2. 选择学习率，我们前面学习过如果一个学习率太小，会导致整个代码运行的相当慢，浪费很多时间，如果选择一个太大的，会导致函数的震荡，以至于无法收敛，所以一般在选择学习率的时候，需要进行“预实验"，先短暂的运行少数的迭代次数之后，看看学习曲线的变化，之后再继续进行。
>虽然很不想承认，但是这周摆烂的天数有点太多了，导致我这周的内容还有很多东西都还没做完，这下真有点难过了，只能尽量学了

2. optionlab：看看具体是如何通过学习曲线来解决学习率的问题。
	1. 进一步解释了为什么不scale变量会导致下降的很慢，这是因为对w求偏导的时候会多出来一项xi，由于这个xi的大小不一样会导致某些特征下降的很快某些特征下降的很慢。
		1. 此外我之前理解z_score归一化存在一些误解，他会归一化一个正态分布的并不能完全保证是在什么范围之内。
3. 网课2：还有一点时间再多学一点吧白蓝已经过多了。
	1. feature engineer：指的是你可以运用你的知识去掉和组装原有的数据然后得到一个可能会更好的数据让你的机器学习更好更快的解决问题。

# 3.24学习日记
1. 网课1：
	1. 多项式回归：当我们发现，尽管只有一个变量，但是单变量线性函数并不能很好的评价我们的结果的时候，我们就可以考虑利用多项式回归了，你可以对x进行一些改造如x²和根号x，来更好的拟合原函数。
2. optionlab：这里只是让你知道有这么一回事还没让你研究怎么选择比较好的特征。
	1. 尽管我们会在这里找到一个x2这样的看起来非线性的数，但是从根本原则上面我们还是在使用线性函数，这是因为我们将x的平方看作是一个新的变量，像是什么所占土地面积之类的一个变量。
	2. 在这些时候我们只需要定义参数就能拟合相当复杂的函数。
>其实也没多少啊这样就把上周的内容学完了！我还是很厉害的。
